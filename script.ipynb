{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename=[]\n",
    "classes=[]\n",
    "filetype=[]\n",
    "magic_type=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_support=['.py','.pdf','.ics','.html','.ics','.vcs','.xlsx','.xls','.doc','.docx','.txt','.pptx','.ppt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root, directories, filenames in os.walk('./data/'):\n",
    "     for file in filenames:\n",
    "            if file != \".DS_Store\":\n",
    "                file_name, file_extension = os.path.splitext(file)\n",
    "                filename.append(\"{}/{}\".format(root,file))\n",
    "                classes.append(root.split(\"/\")[-1])\n",
    "                filetype.append(file_extension) \n",
    "                magic_type.append(magic.from_file(\"{}/{}\".format(root,file), mime=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['file']=filename\n",
    "df['type']=filetype\n",
    "df['magic_type']=magic_type\n",
    "df['classes']=classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['message/news', 'text/plain', 'text/x-fortran', 'message/rfc822',\n",
       "       'application/octet-stream', 'text/x-c', 'text/x-makefile',\n",
       "       'text/x-c++', 'text/x-lisp', 'application/pdf', 'text/calendar',\n",
       "       'text/x-python', 'application/zip', 'application/vnd.ms-excel',\n",
       "       'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
       "       'inode/x-empty', 'application/msword', 'application/x-bzip2',\n",
       "       'application/vnd.ms-powerpoint',\n",
       "       'application/vnd.openxmlformats-officedocument.presentationml.presentation',\n",
       "       'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n",
       "       'text/html', 'video/webm'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.magic_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>type</th>\n",
       "      <th>magic_type</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/20_newsgroup/alt.atheism/53341</td>\n",
       "      <td></td>\n",
       "      <td>message/news</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/20_newsgroup/alt.atheism/49960</td>\n",
       "      <td></td>\n",
       "      <td>message/news</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/20_newsgroup/alt.atheism/51060</td>\n",
       "      <td></td>\n",
       "      <td>message/news</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/20_newsgroup/alt.atheism/51119</td>\n",
       "      <td></td>\n",
       "      <td>text/plain</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/20_newsgroup/alt.atheism/51120</td>\n",
       "      <td></td>\n",
       "      <td>message/news</td>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file type    magic_type      classes\n",
       "0  ./data/20_newsgroup/alt.atheism/53341       message/news  alt.atheism\n",
       "1  ./data/20_newsgroup/alt.atheism/49960       message/news  alt.atheism\n",
       "2  ./data/20_newsgroup/alt.atheism/51060       message/news  alt.atheism\n",
       "3  ./data/20_newsgroup/alt.atheism/51119         text/plain  alt.atheism\n",
       "4  ./data/20_newsgroup/alt.atheism/51120       message/news  alt.atheism"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['./data/Presentations/renepal-infosession.pptx',\n",
       "       './data/Presentations/Search.pptx'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['magic_type']=='application/vnd.openxmlformats-officedocument.presentationml.presentation']['file'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileWriter, PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "PDFSyntaxError",
     "evalue": "No /Root object! - Is this really a PDF?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-15166f09afc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_pdf_to_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-3f3770918585>\u001b[0m in \u001b[0;36mconvert_pdf_to_txt\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpagenos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mPDFPage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpagenos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxpages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxpages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_extractable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pdfminer/pdfpage.py\u001b[0m in \u001b[0;36mget_pages\u001b[0;34m(klass, fp, pagenos, maxpages, password, caching, check_extractable)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Create a PDF document object that stores the document structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Check if the document allows text extraction. If not, abort.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_extractable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_extractable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pdfminer/pdfdocument.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPDFSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No /Root object! - Is this really a PDF?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mLITERAL_CATALOG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTRICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?"
     ]
    }
   ],
   "source": [
    "txt=convert_pdf_to_txt(df['file'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docx parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "my_text = docx2txt.process(df[df['type']=='.docx']['file'].values[0])\n",
    "print(my_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import textract\n",
    "txt_doc = textract.process(df[df['type']=='.doc']['file'].values[0], method='antiword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPTX parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_ppt = textract.process(df[df['type']=='.pptx']['file'].values[0], method='python-pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt_ppt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPT parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.9958458546933"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82906596444740432"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83523632501327671"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(newsgroups_test.target, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', str(text_string))\n",
    "    #text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return(text_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = [document.lower().replace('\\n', '').split() for document in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data_train=[]\n",
    "for data in newsgroups_train.data:\n",
    "    text_data_train.append(clean_text(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ' '.join(text_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data_test=[]\n",
    "for data in newsgroups_test.data:\n",
    "    text_data_test.append(clean_text(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 118020)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = vectorizer.fit_transform(text_data_train)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83871602964644809"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_test = vectorizer.transform(text_data_test)\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(vectors, newsgroups_train.target)\n",
    "pred = clf.predict(vectors_test)\n",
    "metrics.f1_score(newsgroups_test.target, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84599044078597985"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(newsgroups_test.target, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.21.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from os import path\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "google_news_word2vec_model_location = 'data/GoogleNews-vectors-negative300.bin.gz'\n",
    "doc2vec_model_location = 'model/doc2vec-model.bin'\n",
    "doc2vec_vectors_location = 'model/doc2vec-vectors.bin'\n",
    "doc2vec_dimensions = 300\n",
    "classifier_model_location = 'model/classifier-model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc=[]\n",
    "filename=[]\n",
    "classes=[]\n",
    "filetype=[]\n",
    "magic_type=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for root, directories, filenames in os.walk('./20_newsgroup/'):\n",
    "     for file in filenames:\n",
    "            if file != \".DS_Store\":\n",
    "                file_name, file_extension = os.path.splitext(file)\n",
    "                filename.append(\"{}/{}\".format(root,file))\n",
    "                classes.append(root.split(\"/\")[-1])\n",
    "                # filetype.append(file_extension) \n",
    "                magic_type.append(magic.from_file(\"{}/{}\".format(root,file), mime=True))\n",
    "                file1 = open(\"{}/{}\".format(root,file),\"r\",encoding='utf-8', errors='ignore')\n",
    "                doc.append(file1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13736"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13736"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['doc']=doc\n",
    "df['classes']=classes\n",
    "df['file']=filename\n",
    "#df['type']=filetype\n",
    "df['magic_type']=magic_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>classes</th>\n",
       "      <th>file</th>\n",
       "      <th>magic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu talk.abortion:...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>./20_newsgroup/alt.atheism/53341</td>\n",
       "      <td>message/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>./20_newsgroup/alt.atheism/49960</td>\n",
       "      <td>message/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>./20_newsgroup/alt.atheism/51060</td>\n",
       "      <td>message/news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newsgroups: alt.atheism\\nPath: cantaloupe.srv....</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>./20_newsgroup/alt.atheism/51119</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>./20_newsgroup/alt.atheism/51120</td>\n",
       "      <td>message/news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc      classes  \\\n",
       "0  Xref: cantaloupe.srv.cs.cmu.edu talk.abortion:...  alt.atheism   \n",
       "1  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49...  alt.atheism   \n",
       "2  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...  alt.atheism   \n",
       "3  Newsgroups: alt.atheism\\nPath: cantaloupe.srv....  alt.atheism   \n",
       "4  Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:51...  alt.atheism   \n",
       "\n",
       "                               file    magic_type  \n",
       "0  ./20_newsgroup/alt.atheism/53341  message/news  \n",
       "1  ./20_newsgroup/alt.atheism/49960  message/news  \n",
       "2  ./20_newsgroup/alt.atheism/51060  message/news  \n",
       "3  ./20_newsgroup/alt.atheism/51119    text/plain  \n",
       "4  ./20_newsgroup/alt.atheism/51120  message/news  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13736"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data=df['doc'].values\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the reuters news articles and convert them to TaggedDocuments\n",
    "taggedDocuments = [TaggedDocument(words=word_tokenize(fileId), tags=[i]) for i, fileId in enumerate(all_data)]\n",
    "shuffle(taggedDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['Xref', ':', 'cantaloupe.srv.cs.cmu.edu', 'comp.multimedia:6543', 'comp.graphics:38390', 'Path', ':', 'cantaloupe.srv.cs.cmu.edu', '!', 'das-news.harvard.edu', '!', 'ogicse', '!', 'usenet.ee.pdx.edu', '!', 'pdxgate', '!', 'rigel', '!', 'idr', 'From', ':', 'idr', '@', 'rigel.cs.pdx.edu', '(', 'Ian', 'D', 'Romanick', ')', 'Newsgroups', ':', 'comp.multimedia', ',', 'comp.graphics', 'Subject', ':', 'Re', ':', 'Rumours', 'about', '3DO', '?', '?', '?', 'Message-ID', ':', '<', '7272', '@', 'pdxgate.UUCP', '>', 'Date', ':', '17', 'Apr', '93', '20:54:46', 'GMT', 'Article-I.D', '.', ':', 'pdxgate.7272', 'References', ':', '<', '1993Mar31.074502.3590', '@', 'aragorn.unibe.ch', '>', '<', '1993Apr15.143444.32980', '@', 'rchland.ibm.com', '>', '<', '1993Apr15.143444.32980', '@', 'rchland.ibm.com', '>', '<', '1993Apr15.164940.11632', '@', 'mercury.unt.edu', '>', 'Sender', ':', 'news', '@', 'pdxgate.UUCP', 'Organization', ':', 'Portland', 'State', 'University', ',', 'Computer', 'Science', 'Dept', '.', 'Lines', ':', '20', 'In', 'article', '<', '1993Apr15.164940.11632', '@', 'mercury.unt.edu', '>', 'mcmains', '@', 'unt.edu', '(', 'Sean', 'McMains', ')', 'writes', ':', '>', '>', 'Wow', '!', 'A', '68070', '!', 'I', \"'d\", 'be', 'very', 'interested', 'to', 'get', 'my', 'hands', 'on', 'one', 'of', 'these', ',', '>', 'especially', 'considering', 'the', 'fact', 'that', 'Motorola', 'has', 'not', 'yet', 'released', 'the', '>', '68060', ',', 'which', 'is', 'supposedly', 'the', 'next', 'in', 'the', '680x0', 'lineup', '.', '8-D', 'A', '68070', 'is', 'just', 'a', '68010', 'with', 'a', 'built', 'in', 'MMU', '.', 'I', 'do', \"n't\", 'even', 'think', 'that', 'Moto', '.', 'manufactures', 'them', '.', '-', 'Ian', 'Romanick', 'Dancing', 'Fool', 'of', 'Epsilon', '[', ']', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '[', ']', '|', 'Were', 'the', 'contained', 'thoughts', \"'opinions\", \"'\", ',', 'EPN.NTSC.quality', '=', 'Best|', '|', 'PSU', 'would', 'probably', 'not', 'agree', 'with', 'them', '.', '|', '|', '|', '|', '``', 'Look', ',', 'I', 'do', \"n't\", 'know', 'anything', 'about', '|', '|', 'douche', ',', 'but', 'I', 'do', 'know', 'Anti-Freeze', '|', '|', 'when', 'I', 'see', 'it', '!', \"''\", '-', 'The', 'Dead', 'Milkmen', '|', '[', ']', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '[', ']'], tags=[1096])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedDocuments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create and train the doc2vec model\n",
    "doc2vec = Doc2Vec(size=doc2vec_dimensions, min_count=2, iter=10, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the word2vec model from the corpus\n",
    "doc2vec.build_vocab(taggedDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39235530"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.train(taggedDocuments,total_examples=doc2vec.corpus_count,epochs=doc2vec.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec.save(\"doc2vec_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec_model_location = 'doc2vec_new'\n",
    "doc2vec_dimensions = 300\n",
    "\n",
    "doc2vec1 = Doc2Vec.load(doc2vec_model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_model_location = 'classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing.label import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 doc  target\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...      10\n",
       "1  My brother is in the market for a high-perform...       3\n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...      17\n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...       3\n",
       "4  1)    I have an old Jasmine drive which I cann...       4"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=pd.get_dummies(df['classes'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['doc'].values,y.values , test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the categories to one hot encoded categories\n",
    "labelBinarizer = MultiLabelBinarizer()\n",
    "labelBinarizer.fit([df['classes'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the articles to document vectors using the doc2vec model\n",
    "train_data = [doc2vec1.infer_vector(word_tokenize(article)) for article in X_train]\n",
    "test_data = [doc2vec1.infer_vector(word_tokenize(article)) for article in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = np.asarray(train_data), np.asarray(test_data), np.asarray(y_train), np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishikesh/.local/lib/python3.5/site-packages/ipykernel/__main__.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(input_dim=300, units=500, activation=\"relu\")`\n",
      "  app.launch_new_instance()\n",
      "/home/rishikesh/.local/lib/python3.5/site-packages/ipykernel/__main__.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1200, activation=\"relu\")`\n",
      "/home/rishikesh/.local/lib/python3.5/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=400, activation=\"relu\")`\n",
      "/home/rishikesh/.local/lib/python3.5/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=600, activation=\"relu\")`\n",
      "/home/rishikesh/.local/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=20, activation=\"sigmoid\")`\n"
     ]
    }
   ],
   "source": [
    "# Initialize the neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=doc2vec_dimensions, output_dim=500, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_dim=1200, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_dim=400, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_dim=600, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_dim=train_labels.shape[1], activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12626 samples, validate on 6220 samples\n",
      "Epoch 1/15\n",
      "12320/12626 [============================>.] - ETA: 0s - loss: 2.2275 - acc: 0.2616Epoch 00000: val_loss improved from inf to 1.92243, saving model to classifier\n",
      "12626/12626 [==============================] - 1s - loss: 2.2197 - acc: 0.2650 - val_loss: 1.9224 - val_acc: 0.3847\n",
      "Epoch 2/15\n",
      "12128/12626 [===========================>..] - ETA: 0s - loss: 1.8769 - acc: 0.3792Epoch 00001: val_loss improved from 1.92243 to 1.86960, saving model to classifier\n",
      "12626/12626 [==============================] - 1s - loss: 1.8791 - acc: 0.3779 - val_loss: 1.8696 - val_acc: 0.3895\n",
      "Epoch 3/15\n",
      "12576/12626 [============================>.] - ETA: 0s - loss: 1.7687 - acc: 0.4133Epoch 00002: val_loss improved from 1.86960 to 1.81287, saving model to classifier\n",
      "12626/12626 [==============================] - 1s - loss: 1.7689 - acc: 0.4130 - val_loss: 1.8129 - val_acc: 0.4027\n",
      "Epoch 4/15\n",
      "12320/12626 [============================>.] - ETA: 0s - loss: 1.6784 - acc: 0.4424Epoch 00003: val_loss improved from 1.81287 to 1.76648, saving model to classifier\n",
      "12626/12626 [==============================] - 1s - loss: 1.6795 - acc: 0.4417 - val_loss: 1.7665 - val_acc: 0.4243\n",
      "Epoch 5/15\n",
      "12480/12626 [============================>.] - ETA: 0s - loss: 1.6143 - acc: 0.4536Epoch 00004: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.6153 - acc: 0.4538 - val_loss: 1.8037 - val_acc: 0.4133\n",
      "Epoch 6/15\n",
      "12544/12626 [============================>.] - ETA: 0s - loss: 1.5515 - acc: 0.4728Epoch 00005: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.5525 - acc: 0.4722 - val_loss: 1.8042 - val_acc: 0.4264\n",
      "Epoch 7/15\n",
      "12448/12626 [============================>.] - ETA: 0s - loss: 1.4963 - acc: 0.4961Epoch 00006: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.4962 - acc: 0.4960 - val_loss: 1.7837 - val_acc: 0.4376\n",
      "Epoch 8/15\n",
      "12608/12626 [============================>.] - ETA: 0s - loss: 1.4235 - acc: 0.5061Epoch 00007: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.4232 - acc: 0.5063 - val_loss: 1.8409 - val_acc: 0.4357\n",
      "Epoch 9/15\n",
      "12544/12626 [============================>.] - ETA: 0s - loss: 1.3743 - acc: 0.5252Epoch 00008: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.3757 - acc: 0.5251 - val_loss: 1.8234 - val_acc: 0.4222\n",
      "Epoch 10/15\n",
      "12448/12626 [============================>.] - ETA: 0s - loss: 1.3265 - acc: 0.5411Epoch 00009: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.3251 - acc: 0.5413 - val_loss: 1.8641 - val_acc: 0.4494\n",
      "Epoch 11/15\n",
      "12416/12626 [============================>.] - ETA: 0s - loss: 1.2916 - acc: 0.5496Epoch 00010: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.2879 - acc: 0.5508 - val_loss: 1.8797 - val_acc: 0.4293\n",
      "Epoch 12/15\n",
      "12416/12626 [============================>.] - ETA: 0s - loss: 1.2290 - acc: 0.5730Epoch 00011: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.2297 - acc: 0.5725 - val_loss: 1.9273 - val_acc: 0.4392\n",
      "Epoch 13/15\n",
      "12384/12626 [============================>.] - ETA: 0s - loss: 1.1901 - acc: 0.5789Epoch 00012: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.1926 - acc: 0.5772 - val_loss: 1.9117 - val_acc: 0.4410\n",
      "Epoch 14/15\n",
      "12416/12626 [============================>.] - ETA: 0s - loss: 1.1636 - acc: 0.5921Epoch 00013: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.1653 - acc: 0.5921 - val_loss: 1.9537 - val_acc: 0.4309\n",
      "Epoch 15/15\n",
      "12384/12626 [============================>.] - ETA: 0s - loss: 1.1224 - acc: 0.5931Epoch 00014: val_loss did not improve\n",
      "12626/12626 [==============================] - 1s - loss: 1.1225 - acc: 0.5928 - val_loss: 2.0508 - val_acc: 0.4191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5bca11908>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saves the model with highest score after each training cycle\n",
    "checkpointer = ModelCheckpoint(filepath=classifier_model_location, verbose=1, save_best_only=True)\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(train_data, train_labels, validation_data=(test_data, test_labels), batch_size=32, nb_epoch=15, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
